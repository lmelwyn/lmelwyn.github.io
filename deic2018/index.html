<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="revealjs/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="revealjs/css/theme/white.css" id="theme">
<link rel="stylesheet" href="style/custom.css">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'revealjs/css/print/pdf.css' : 'revealjs/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="revealjs/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="katex/katex.min.js"></script><link rel="stylesheet" href="katex/katex.min.css" /><script type="text/javascript">window.onload = function(){var mathElements = document.getElementsByClassName("math");
  for (var i=0; i < mathElements.length; i++)
  {
   var texText = mathElements[i].firstChild
   katex.render(texText.data, mathElements[i])
  }}
  </script>

<script src="//twemoji.maxcdn.com/2/twemoji.min.js?2.4"></script>
<script>
  function addEvent(element, eventName, fn) {
    if (element.addEventListener)
      element.addEventListener(eventName, fn, false);
    else if (element.attachEvent)
      element.attachEvent('on' + eventName, fn);
  }

  addEvent(window, 'load', function() {
    twemoji.parse(document.body, {'folder': 'svg', 'ext': '.svg'});
  });
</script>

</head>
<body>
  <div class="reveal">
<div class="logo"><img src="style/logo.svg" /></div>
    <div class="slides">


<section data-background="style/background.svg"><section id="ansible-openhpc" class="titleslide slide level1"><h1>Ansible &amp; OpenHPC</h1></section><section id="about-me" class="slide level2">
<h2>About me</h2>
<ul>
<li>Lars Melwyn</li>
<li>HPC officer @ DTU AIT RIT</li>
</ul>
<p><img src="./figures/new-cluster.svg" 
     style="float: right"
     margin-align="left"
     width="355"
     /></p>
<ul>
<li>HPC cluster builder
<ul>
<li>Umeå University, Sweden</li>
<li>NORDITA, Denmark</li>
<li>ESS DMSC, Denmark</li>
<li>Herlev hospital<br />
</li>
<li>DTU RIT, Campus Risø</li>
</ul></li>
</ul>
</section><section id="section" class="slide level2">
<h2></h2>
<p>currently ...</p>
<ul>
<li>DTU AIT RIT, hosting facilities
<ul>
<li>two locations, server room + container(s)</li>
<li>Jess, 320 HP/Intel nodes, 6400 cores</li>
<li>Gorm, 80 Dell/Intel nodes, 1000 cores</li>
<li>Mimers, 13 lustre server 1+ PB<br />
</li>
<li>Ceph, 12 ceph servers, 1+ PB</li>
</ul></li>
</ul>
<p>need more ...</p>
<ul>
<li>DTU AIT + DTU Wind Energy + DTU Mechanical Engineering
<ul>
<li>20 MDKK tender</li>
<li>ATEA/DELL solution</li>
</ul></li>
</ul>
</section><section id="ait-hpc-tender" class="slide level2">
<h2>AIT HPC tender</h2>
<table>
<thead>
<tr class="header">
<th>Type</th>
<th>Count</th>
<th>Model</th>
<th>Specs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Compute 1</td>
<td>490</td>
<td>Dell R7425</td>
<td>2 x EPYC 7351/16C</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>16x8GB RAM, 2x10G</td>
</tr>
<tr class="odd">
<td>Compute 2</td>
<td>32</td>
<td>Dell R7425</td>
<td>2 x EPYC 7351/16C</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>16x16GB RAM, 2x10G</td>
</tr>
<tr class="odd">
<td>Login</td>
<td>2</td>
<td>Dell R7425</td>
<td>2 x EPYC 7351/16C</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>16x8GB RAM, 2x10G</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>2 x P4000 NVIDIA</td>
</tr>
<tr class="even">
<td>Infiniband</td>
<td>49</td>
<td>Mellanox S78xx switches</td>
<td>36 x 100G</td>
</tr>
<tr class="odd">
<td>Ethernet</td>
<td>15</td>
<td>Dell Networking S4048-ON</td>
<td>48 x 10G</td>
</tr>
<tr class="even">
<td>Ethernet</td>
<td>15</td>
<td>Dell Networking S3048</td>
<td>48 x 1G</td>
</tr>
<tr class="odd">
<td>Container</td>
<td>1</td>
<td>Data center</td>
<td>30 racks</td>
</tr>
</tbody>
</table>
</section></section>
<section data-background="style/background.svg"><section id="section-1" class="titleslide slide level1"><h1><img src="./figures/ansible_logo.svg"  style="float: center; margin-center: 10px;" /></h1></section><section id="descriptive-language-for-admin-tasks" class="slide level2">
<h2>Descriptive language for admin tasks</h2>
<ul>
<li>Installing packages</li>
<li>Generating conf files</li>
<li>Managing file system permissions</li>
<li>Setting environment variables</li>
<li>Restarting daemons/services</li>
<li>Updating code releases</li>
<li>Managing firewall rules</li>
<li>.... and much more</li>
</ul>
</section><section id="day-course-...-in-10-min" class="slide level2">
<h2>4 day course ... in 10 min</h2>
<ul>
<li>Course introduction</li>
<li>Introduce Ansible</li>
<li>Deploy Ansible</li>
<li>Implement playbooks</li>
<li>Manage variables and inclusions</li>
<li>Implement task control</li>
<li>Implement Jinja2 templates</li>
<li>Implement roles</li>
<li>Configure complex playbooks</li>
<li>Implement Ansible Vault</li>
<li>Troubleshoot Ansible</li>
<li>Implement Ansible Tower</li>
<li>Implement Ansible in a DevOps environment</li>
<li>Comprehensive review</li>
</ul>
</section><section id="ansible-design-goals" class="slide level2">
<h2>Ansible design goals</h2>
<ul>
<li>Minimal in nature. Only OpenSSH and a bit of Python and YAML</li>
<li>Highly reliable for consistent and reproducible environments</li>
<li>Secure and agentless</li>
<li>OS / distribution agnostic<br />
</li>
<li>Idempotent, <span class="math inline">T(T(x))=T(x)</span>, when carefully programmed</li>
<li>Shallow learning curve, 10 minutes and you are good to go</li>
<li>Easy to use</li>
<li>Descriptive YAML (state)</li>
<li>Jinja templates (simple variable replacement + much more)</li>
<li>Flexible setup</li>
</ul>
</section><section id="concepts" class="slide level2">
<h2>Concepts</h2>
<p><img src="./figures/ansible_works.svg" 
     style="float: right"
     margin-align="left"
     width="400"
     /></p>
<ul>
<li><span style="color:dodgerblue"><strong>Inventory</strong></span>
<ul>
<li>devices</li>
<li>device group</li>
</ul></li>
<li><span style="color:dodgerblue"><strong>Task</strong></span>
<ul>
<li>single task = raw command</li>
<li>multiple tasks = <span style="color:dodgerblue"><strong>playbook</strong></span></li>
<li>organized tasks = <span style="color:dodgerblue"> <strong>role</strong></span></li>
</ul></li>
<li><span style="color:dodgerblue"><strong>Modules</strong></span>
<ul>
<li>task / state functions</li>
<li>variables</li>
</ul></li>
</ul>
</section><section id="simple-playbook-task" class="slide level2">
<h2>Simple playbook task</h2>
<figure>
<img src="./figures/task-2.png" width="680" />
</figure>
<p>Descriptive, system state oriented, module and jinja</p>
</section><section id="modules" class="slide level2">
<h2>Modules</h2>
<ul>
<li>list of <a href="https://docs.ansible.com/ansible/latest/modules/modules_by_category.html">modules</a>
<ul>
<li><a href="https://docs.ansible.com/ansible/latest/modules/list_of_packaging_modules.html">packaging modules</a></li>
<li><a href="https://docs.ansible.com/ansible/latest/modules/list_of_cloud_modules.html">cloud modules</a></li>
<li><a href="https://docs.ansible.com/ansible/latest/modules/list_of_network_modules.html">network modules</a></li>
<li>....</li>
</ul></li>
</ul>
</section><section id="playbook-language" class="slide level2">
<h2>Playbook language</h2>
<ul>
<li><a href="https://docs.ansible.com/ansible/2.7/user_guide/intro_adhoc.html">ad-hoc</a></li>
<li><a href="https://docs.ansible.com/ansible/2.7/user_guide/playbooks_loops.html">loops</a></li>
<li><a href="https://docs.ansible.com/ansible/2.7/user_guide/playbooks_conditionals.html">conditionals</a></li>
<li><a href="https://docs.ansible.com/ansible/2.7/user_guide/playbooks_templating.html">jinja2 templates</a></li>
</ul>
</section><section id="roles" class="slide level2">
<h2>Roles</h2>
<figure>
<img src="./figures/create-role.png" />
</figure>
</section><section id="role-based-playbook" class="slide level2">
<h2>Role based playbook</h2>
<ul>
<li>ansible-playbook -i inventory/all playbook/melwyn.repos.yml --user=root</li>
</ul>
<figure>
<img src="./figures/ansible-repo3.png" width="700" />
</figure>
<ul>
<li>--limit host1</li>
</ul>
</section><section id="tasksredhat.yml" class="slide level2">
<h2>tasks/redhat.yml</h2>
<figure>
<img src="./figures/ansible-repo1.png" width="700" />
</figure>
</section><section id="defaultsmain.yml" class="slide level2">
<h2>defaults/main.yml</h2>
<figure>
<img src="./figures/ansible-repo2.png" width="700" />
</figure>
</section><section id="ansible-doc" class="slide level2">
<h2>Ansible-doc</h2>
<figure>
<img src="./figures/ansible-doc.png" />
</figure>
</section><section id="galaxy.ansible.com" class="slide level2">
<h2>galaxy.ansible.com</h2>
<figure>
<img src="./figures/galaxy.png" width="800" />
</figure>
</section><section id="more-commands" class="slide level2">
<h2>more commands</h2>
<figure>
<img src="./figures/more-features.png" />
</figure>
</section><section id="final-remarks" class="slide level2">
<h2>Final remarks</h2>
<ul>
<li><span style="color:dodgerblue">Ansible is workflow automation for </span>
<ul>
<li>provisioning</li>
<li>deployment<br />
</li>
<li>management</li>
</ul></li>
<li><span style="color:dodgerblue">Turn bare metal into software defined!</span>
<ul>
<li>service-not-function oriented</li>
<li>alternativ to virtualization vmware/openstack</li>
</ul></li>
<li><span style="color:dodgerblue">Redhat owned project</span>
<ul>
<li>good <a href="https://docs.ansible.com">documentation</a></li>
<li>large community, <a href="https://groups.google.com/forum/#!forum/ansible-project">mailling lists</a></li>
<li>ansible tower web based application</li>
</ul></li>
<li><span style="color:dodgerblue">Outlook</span>
<ul>
<li>can be used for much more than sysadm</li>
<li>gitlab ci, gitlab-runner</li>
</ul></li>
</ul>
</section></section>
<section data-background="style/background.svg"><section id="section-2" class="titleslide slide level1"><h1><img src="./figures/openhpc_logo.png"  style="float: bottom; margin-center: 10px;" /></h1></section><section id="standard-hpc-cluster-design" class="slide level2">
<h2>Standard HPC cluster design</h2>
<figure>
<img src="./figures/ohpc-cluster.png" />
</figure>
</section><section id="general-hpc-software-stack" class="slide level2">
<h2>General HPC software stack</h2>
<figure>
<img src="./figures/hpc-stack.png" />
</figure>
<ul>
<li>pxe/kickstart &amp; do it yourself -&gt; done</li>
</ul>
</section><section id="what-is-openhpc" class="slide level2">
<h2>What is OpenHPC?</h2>
<figure>
<img src="./figures/overview.png" style="width:55.0%" />
</figure>
<ul>
<li>Building blocks for open (source) HPC</li>
</ul>
</section><section id="pick-choose-and-mix-match" class="slide level2">
<h2>Pick &amp; Choose and Mix &amp; Match</h2>
<p><img src="./figures/pick-choose.svg" 
     style="float: right"
     margin-align="left"
     width="375"
     /></p>
<ul>
<li><span style="color:dodgerblue">Provisioning</span>
<ul>
<li>configuration management</li>
</ul></li>
<li><span style="color:dodgerblue">Deployment system</span>
<ul>
<li>warewulf</li>
<li>xCat</li>
<li>losf + cobbler</li>
</ul></li>
<li><span style="color:dodgerblue">Resource management</span>
<ul>
<li>slurm</li>
<li>pbspro</li>
</ul></li>
<li><span style="color:dodgerblue">Software packages</span>
<ul>
<li>ohpc rpm</li>
<li>easybuild</li>
<li>spack</li>
</ul></li>
</ul>
</section><section id="why-openhpc" class="slide level2">
<h2>Why OpenHPC</h2>
<ul>
<li><span style="color:dodgerblue">Standard</span>
<ul>
<li>standard interfaces (lmod, ohpc, easybuild, spack)</li>
<li>extensive documentation</li>
<li>active community</li>
<li>security oriented</li>
<li>build server framework</li>
<li>software validation</li>
<li>CentOS HPC SIG</li>
</ul></li>
<li><span style="color:dodgerblue">Best/Good practice</span>
<ul>
<li>power management</li>
<li>system performance monitoring</li>
<li>application libraries</li>
<li>dev tools</li>
<li>parallel file systems</li>
<li>system utilization monitoring</li>
</ul></li>
</ul>
</section><section id="openhpc-howto" class="slide level2">
<h2>OpenHPC Howto</h2>
<ul>
<li><a href="http://openhpc.community/downloads/">push to start</a></li>
</ul>
<figure>
<img src="./figures/deploy.png" />
</figure>
<ul>
<li><a href="http://warewulf.lbl.gov/">home/warewulf</a>, <a href="https://github.com/warewulf/warewulf3">github/warewulf</a></li>
<li><a href="https://xcat.org/">home/xCat</a>, <a href="https://github.com/xcat2">github/xCat</a></li>
<li><a href="https://github.com/davidcarver/losf-cookbook/wiki">home/losf</a> and <a href="http://cobbler.github.io">home/cobbler</a></li>
</ul>
<p>Simple scripted deployment with warewulf</p>
</section><section id="reference-recipe.sh" class="slide level2">
<h2>Reference recipe.sh</h2>
<pre><code>#!/bin/bash
# -----------------------------------------------------------------------------------------
#  Example Installation Script Template
#  
#  This convenience script encapsulates command-line instructions highlighted in
#  the OpenHPC Install Guide that can be used as a starting point to perform a local
#  cluster install beginning with bare-metal. Necessary inputs that describe local
#  hardware characteristics, desired network settings, and other customizations
#  are controlled via a companion input file that is used to initialize variables 
#  within this script.
#   
#  Please see the OpenHPC Install Guide for more information regarding the
#  procedure. Note that the section numbering included in this script refers to
#  corresponding sections from the install guide.
# -----------------------------------------------------------------------------------------

inputFile=${OHPC_INPUT_LOCAL:-/opt/ohpc/pub/doc/recipes/vanilla/input.local}

if [ ! -e ${inputFile} ];then
   echo &quot;Error: Unable to access local input file -&gt; ${inputFile}&quot;
   exit 1
else
   . ${inputFile} || { echo &quot;Error sourcing ${inputFile}&quot;; exit 1; }
fi

# ---------------------------- Begin OpenHPC Recipe ---------------------------------------
# Commands below are extracted from an OpenHPC install guide recipe and are intended for 
# execution on the master SMS host.
# -----------------------------------------------------------------------------------------

# Verify OpenHPC repository has been enabled before proceeding

yum repolist | grep -q OpenHPC
if [ $? -ne 0 ];then
   echo &quot;Error: OpenHPC repository must be enabled locally&quot;
   exit 1
fi

# ------------------------------------------------------------
# Add baseline OpenHPC and provisioning services (Section 3.3)
# ------------------------------------------------------------
yum -y groupinstall ohpc-base
yum -y groupinstall ohpc-warewulf
# Disable firewall 
systemctl disable firewalld
systemctl stop firewalld
# Enable NTP services on SMS host
systemctl enable ntpd.service
echo &quot;server ${ntp_server}&quot; &gt;&gt; /etc/ntp.conf
systemctl restart ntpd

# -------------------------------------------------------------
# Add resource management services on master node (Section 3.4)
# -------------------------------------------------------------
yum -y groupinstall ohpc-slurm-server
useradd slurm

# ------------------------------------------------------------
# Add InfiniBand support services on master node (Section 3.5)
# ------------------------------------------------------------
yum -y groupinstall &quot;InfiniBand Support&quot;
yum -y install infinipath-psm
systemctl start rdma

if [[ ${enable_ipoib} -eq 1 ]];then
     # Enable ib0
     cp /opt/ohpc/pub/examples/network/centos/ifcfg-ib0 /etc/sysconfig/network-scripts
     perl -pi -e &quot;s/master_ipoib/${sms_ipoib}/&quot; /etc/sysconfig/network-scripts/ifcfg-ib0
     perl -pi -e &quot;s/ipoib_netmask/${ipoib_netmask}/&quot; /etc/sysconfig/network-scripts/ifcfg-ib0
     ifup ib0
fi

# Optionally enable opensm subnet manager
if [[ ${enable_opensm} -eq 1 ]];then
     yum -y install opensm
     systemctl enable opensm
     systemctl start opensm
fi

# -----------------------------------------------------------
# Complete basic Warewulf setup for master node (Section 3.6)
# -----------------------------------------------------------
perl -pi -e &quot;s/device = eth1/device = ${sms_eth_internal}/&quot; /etc/warewulf/provision.conf
perl -pi -e &quot;s/^\s+disable\s+= yes/ disable = no/&quot; /etc/xinetd.d/tftp
export MODFILE=/etc/httpd/conf.d/warewulf-httpd.conf
perl -pi -e &quot;s/cgi-bin&gt;\$/cgi-bin&gt;\n Require all granted/&quot; $MODFILE
perl -pi -e &quot;s/Allow from all/Require all granted/&quot; $MODFILE
perl -ni -e &quot;print unless /^\s+Order allow,deny/&quot; $MODFILE
ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up
systemctl restart xinetd
systemctl enable mariadb.service
systemctl restart mariadb
systemctl enable httpd.service
systemctl restart httpd
if [ ! -z ${BOS_MIRROR+x} ]; then
     perl -pi -e &quot;s#^YUM_MIRROR=(\S+)#YUM_MIRROR=${BOS_MIRROR}#&quot; /usr/libexec/warewulf/wwmkchroot/centos-7.tmpl
fi

# -------------------------------------------------
# Create compute image for Warewulf (Section 3.7.1)
# -------------------------------------------------
export CHROOT=/opt/ohpc/admin/images/centos7.2
wwmkchroot centos-7 $CHROOT

# -------------------------------------------------------
# Add OpenHPC components to compute image (Section 3.7.2)
# -------------------------------------------------------
cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
# Add OpenHPC components to compute instance
yum -y --installroot=$CHROOT groupinstall ohpc-slurm-client
yum -y --installroot=$CHROOT groupinstall &quot;InfiniBand Support&quot;
yum -y --installroot=$CHROOT install infinipath-psm
chroot $CHROOT systemctl enable rdma
yum -y --installroot=$CHROOT install ntp
yum -y --installroot=$CHROOT install kernel
yum -y --installroot=$CHROOT install lmod-ohpc

# ----------------------------------------------
# Customize system configuration (Section 3.7.3)
# ----------------------------------------------
wwinit ssh_keys
cat ~/.ssh/cluster.pub &gt;&gt; $CHROOT/root/.ssh/authorized_keys
echo &quot;${sms_ip}:/home /home nfs nfsvers=3,rsize=1024,wsize=1024,cto 0 0&quot; &gt;&gt; $CHROOT/etc/fstab
echo &quot;${sms_ip}:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3 0 0&quot; &gt;&gt; $CHROOT/etc/fstab
perl -pi -e &quot;s/ControlMachine=\S+/ControlMachine=${sms_name}/&quot; /etc/slurm/slurm.conf
echo &quot;/home *(rw,no_subtree_check,fsid=10,no_root_squash)&quot; &gt;&gt; /etc/exports
echo &quot;/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)&quot; &gt;&gt; /etc/exports
exportfs -a
systemctl restart nfs
systemctl enable nfs-server
chroot $CHROOT systemctl enable ntpd
echo &quot;server ${sms_ip}&quot; &gt;&gt; $CHROOT/etc/ntp.conf

# Update basic slurm configuration if additional computes defined
if [ ${num_computes} -gt 4 ];then
   perl -pi -e &quot;s/^NodeName=(\S+)/NodeName=c[1-${num_computes}]/&quot; /etc/slurm/slurm.conf
   perl -pi -e &quot;s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=c[1-${num_computes}]/&quot; /etc/slurm/slurm.conf
   perl -pi -e &quot;s/^NodeName=(\S+)/NodeName=c[1-${num_computes}]/&quot; $CHROOT/etc/slurm/slurm.conf
   perl -pi -e &quot;s/^PartitionName=normal Nodes=(\S+)/PartitionName=normal Nodes=c[1-${num_computes}]/&quot; $CHROOT/etc/slurm/slurm.conf
fi

# -----------------------------------------
# Additional customizations (Section 3.7.4)
# -----------------------------------------
perl -pi -e &#39;s/# End of file/\* soft memlock unlimited\n$&amp;/s&#39; /etc/security/limits.conf
perl -pi -e &#39;s/# End of file/\* hard memlock unlimited\n$&amp;/s&#39; /etc/security/limits.conf
perl -pi -e &#39;s/# End of file/\* soft memlock unlimited\n$&amp;/s&#39; $CHROOT/etc/security/limits.conf
perl -pi -e &#39;s/# End of file/\* hard memlock unlimited\n$&amp;/s&#39; $CHROOT/etc/security/limits.conf
# Enable slurm pam module
echo &quot;account    required     pam_slurm.so&quot; &gt;&gt; $CHROOT/etc/pam.d/sshd

# Enable Optional packages

if [[ ${enable_lustre_client} -eq 1 ]];then
     # Install Lustre client on master
     yum -y install lustre-client-ohpc lustre-client-ohpc-modules
     # Enable lustre in WW compute image
     yum -y --installroot=$CHROOT install lustre-client-ohpc lustre-client-ohpc-modules
     mkdir $CHROOT/mnt/lustre
     echo &quot;${mgs_fs_name} /mnt/lustre lustre defaults,_netdev,localflock 0 0&quot; &gt;&gt; $CHROOT/etc/fstab
     # Enable o2ib for Lustre
     echo &quot;options lnet networks=o2ib(ib0)&quot; &gt;&gt; /etc/modprobe.d/lustre.conf
     echo &quot;options lnet networks=o2ib(ib0)&quot; &gt;&gt; $CHROOT/etc/modprobe.d/lustre.conf
     # mount Lustre client on master
     mkdir /mnt/lustre
     mount -t lustre -o localflock ${mgs_fs_name} /mnt/lustre
fi

if [[ ${enable_nagios} -eq 1 ]];then
     # Install Nagios on master and vnfs image
     yum -y groupinstall ohpc-nagios
     yum -y --installroot=$CHROOT groupinstall ohpc-nagios
     chroot $CHROOT systemctl enable nrpe
     perl -pi -e &quot;s/^allowed_hosts=/# allowed_hosts=/&quot; $CHROOT/etc/nagios/nrpe.cfg
     echo &quot;nrpe 5666/tcp # NRPE&quot;         &gt;&gt; $CHROOT/etc/services
     echo &quot;nrpe : ${sms_ip}  : ALLOW&quot;    &gt;&gt; $CHROOT/etc/hosts.allow
     echo &quot;nrpe : ALL : DENY&quot;            &gt;&gt; $CHROOT/etc/hosts.allow
     chroot $CHROOT /usr/sbin/useradd -c &quot;NRPE user for the NRPE service&quot; -d /var/run/nrpe -r -g nrpe -s /sbin/nologin nrpe
     chroot $CHROOT /usr/sbin/groupadd -r nrpe
     mv /etc/nagios/conf.d/services.cfg.example /etc/nagios/conf.d/services.cfg
     mv /etc/nagios/conf.d/hosts.cfg.example /etc/nagios/conf.d/hosts.cfg
     for ((i=0; i&lt;$num_computes; i++)) ; do
        perl -pi -e &quot;s/HOSTNAME$(($i+1))/${c_name[$i]}/ || s/HOST$(($i+1))_IP/${c_ip[$i]}/&quot; \
        /etc/nagios/conf.d/hosts.cfg
     done
     perl -pi -e &quot;s/ \/bin\/mail/ \/usr\/bin\/mailx/g&quot; /etc/nagios/objects/commands.cfg
     perl -pi -e &quot;s/nagios\@localhost/root\@${sms_name}/&quot; /etc/nagios/objects/contacts.cfg
     echo command[check_ssh]=/usr/lib64/nagios/plugins/check_ssh localhost &gt;&gt; $CHROOT/etc/nagios/nrpe.cfg
     chkconfig nagios on
     systemctl start nagios
     chmod u+s `which ping`
fi

if [[ ${enable_ganglia} -eq 1 ]];then
     # Install Ganglia on master
     yum -y groupinstall ohpc-ganglia
     yum -y --installroot=$CHROOT install ganglia-gmond-ohpc
     cp /opt/ohpc/pub/examples/ganglia/gmond.conf /etc/ganglia/gmond.conf
     perl -pi -e &quot;s/&lt;sms&gt;/${sms_name}/&quot; /etc/ganglia/gmond.conf
     cp /etc/ganglia/gmond.conf $CHROOT/etc/ganglia/gmond.conf
     echo &quot;gridname MySite&quot; &gt;&gt; /etc/ganglia/gmetad.conf
     systemctl enable gmond
     systemctl enable gmetad
     systemctl start gmond
     systemctl start gmetad
     chroot $CHROOT systemctl enable gmond
     systemctl try-restart httpd
fi

if [[ ${enable_clustershell} -eq 1 ]];then
     # Install clustershell
     yum -y install clustershell-ohpc
     cd /etc/clustershell/groups.d
     mv local.cfg local.cfg.orig
     echo &quot;adm: ${sms_name}&quot; &gt; local.cfg
     echo &quot;compute: c[1-${num_computes}]&quot; &gt;&gt; local.cfg
     echo &quot;all: @adm,@compute&quot; &gt;&gt; local.cfg
fi

if [[ ${enable_mrsh} -eq 1 ]];then
     # Install mrsh
     yum -y install mrsh-ohpc mrsh-rsh-compat-ohpc
     yum -y --installroot=$CHROOT install mrsh-ohpc mrsh-rsh-compat-ohpc mrsh-server-ohpc
     echo &quot;mshell          21212/tcp                  # mrshd&quot; &gt;&gt; /etc/services
     echo &quot;mlogin            541/tcp                  # mrlogind&quot; &gt;&gt; /etc/services
     chroot $CHROOT systemctl enable xinetd
fi

if [[ ${enable_genders} -eq 1 ]];then
     # Install genders
     yum -y install genders-ohpc
     echo -e &quot;${sms_name}\tsms&quot; &gt; /etc/genders
     for ((i=0; i&lt;$num_computes; i++)) ; do
        echo -e &quot;${c_name[$i]}\tcompute,bmc=${c_bmc[$i]}&quot;
     done &gt;&gt; /etc/genders
fi

# Optionally, enable conman and configure
if [[ ${enable_ipmisol} -eq 1 ]];then
     yum -y install conman-ohpc
     for ((i=0; i&lt;$num_computes; i++)) ; do
        echo -n &#39;CONSOLE name=&quot;&#39;${c_name[$i]}&#39;&quot; dev=&quot;ipmi:&#39;${c_bmc[$i]}&#39;&quot; &#39;
        echo &#39;ipmiopts=&quot;&#39;U:${bmc_username},P:${IPMI_PASSWORD:-undefined},W:solpayloadsize&#39;&quot;&#39;
     done &gt;&gt; /etc/conman.conf
     systemctl enable conman
     systemctl start conman
fi

# --------------------------------------------------------
# Configure rsyslog on SMS and computes (Section 3.7.4.10)
# --------------------------------------------------------
perl -pi -e &quot;s/\\#\\\$ModLoad imudp/\\\$ModLoad imudp/&quot; /etc/rsyslog.conf
perl -pi -e &quot;s/\\#\\\$UDPServerRun 514/\\\$UDPServerRun 514/&quot; /etc/rsyslog.conf
systemctl restart rsyslog
echo &quot;*.* @${sms_ip}:514&quot; &gt;&gt; $CHROOT/etc/rsyslog.conf
perl -pi -e &quot;s/^\*\.info/\\#\*\.info/&quot; $CHROOT/etc/rsyslog.conf
perl -pi -e &quot;s/^authpriv/\\#authpriv/&quot; $CHROOT/etc/rsyslog.conf
perl -pi -e &quot;s/^mail/\\#mail/&quot; $CHROOT/etc/rsyslog.conf
perl -pi -e &quot;s/^cron/\\#cron/&quot; $CHROOT/etc/rsyslog.conf
perl -pi -e &quot;s/^uucp/\\#uucp/&quot; $CHROOT/etc/rsyslog.conf

# ----------------------------
# Import files (Section 3.7.5)
# ----------------------------
wwsh file import /etc/passwd
wwsh file import /etc/group
wwsh file import /etc/shadow 
wwsh file import /etc/slurm/slurm.conf
wwsh file import /etc/munge/munge.key

if [[ ${enable_ipoib} -eq 1 ]];then
     wwsh file import /opt/ohpc/pub/examples/network/centos/ifcfg-ib0.ww
     wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network-scripts/ifcfg-ib0
fi

# --------------------------------------
# Assemble bootstrap image (Section 3.8)
# --------------------------------------
export WW_CONF=/etc/warewulf/bootstrap.conf
echo &quot;drivers += updates/kernel/&quot; &gt;&gt; $WW_CONF
wwbootstrap `uname -r`
# Assemble VNFS
wwvnfs -y --chroot $CHROOT
# Add hosts to cluster
echo &quot;GATEWAYDEV=${eth_provision}&quot; &gt; /tmp/network.$$
wwsh -y file import /tmp/network.$$ --name network
wwsh -y file set network --path /etc/sysconfig/network --mode=0644 --uid=0
for ((i=0; i&lt;$num_computes; i++)) ; do
   wwsh -y node new ${c_name[i]} --ipaddr=${c_ip[i]} --hwaddr=${c_mac[i]} -D ${eth_provision}
done
# Add hosts to cluster (Cont.)
wwsh -y provision set &quot;${compute_regex}&quot; --vnfs=centos7.2 --bootstrap=`uname -r` --files=dynamic_hosts,passwd,group,shadow,slurm.conf,munge.key,network

# Optionally, add arguments to bootstrap kernel
if [[ ${enable_kargs} ]]; then
   wwsh provision set &quot;${compute_regex}&quot; --kargs=${kargs}
fi

# Restart ganglia services to pick up hostfile changes
if [[ ${enable_ganglia} -eq 1 ]];then
  systemctl restart gmond
  systemctl restart gmetad
fi

# Optionally, define IPoIB network settings (required if planning to mount Lustre over IB)
if [[ ${enable_ipoib} -eq 1 ]];then
     for ((i=0; i&lt;$num_computes; i++)) ; do
        wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
     done
     wwsh -y provision set &quot;${compute_regex}&quot; --fileadd=ifcfg-ib0.ww
fi

systemctl restart dhcpd
wwsh pxe update

# Optionally, enable console redirection 
if [[ ${enable_ipmisol} -eq 1 ]];then
     wwsh -y provision set &quot;${compute_regex}&quot; --kargs &quot;${kargs} console=ttyS1,115200&quot;
fi

# --------------------------------
# Boot compute nodes (Section 3.9)
# --------------------------------
for ((i=0; i&lt;${num_computes}; i++)) ; do
   ipmitool -E -I lanplus -H ${c_bmc[$i]} -U ${bmc_username} chassis power reset
done

# ---------------------------------------
# Install Development Tools (Section 4.1)
# ---------------------------------------
yum -y groupinstall ohpc-autotools
yum -y install valgrind-ohpc
yum -y install EasyBuild-ohpc
yum -y install spack-ohpc
yum -y install R_base-ohpc            

# -------------------------------
# Install Compilers (Section 4.2)
# -------------------------------
yum -y install gnu-compilers-ohpc

# --------------------------------
# Install MPI Stacks (Section 4.3)
# --------------------------------
if [[ ${enable_mpi_defaults} -eq 1 ]];then
     yum -y install openmpi-gnu-ohpc mvapich2-gnu-ohpc mpich-gnu-ohpc
elif [[ ${enable_mpi_opa} -eq 1 ]];then
     yum -y install openmpi-psm2-gnu-ohpc mvapich2-psm2-gnu-ohpc
fi

# ---------------------------------------
# Install Performance Tools (Section 4.4)
# ---------------------------------------
yum -y groupinstall ohpc-perf-tools-gnu
yum -y install lmod-defaults-gnu-mvapich2-ohpc

# ---------------------------------------------------
# Install 3rd Party Libraries and Tools (Section 4.6)
# ---------------------------------------------------
yum -y groupinstall ohpc-serial-libs-gnu
yum -y groupinstall ohpc-io-libs-gnu
yum -y groupinstall ohpc-python-libs-gnu
yum -y groupinstall ohpc-runtimes-gnu
if [[ ${enable_mpi_defaults} -eq 1 ]];then
     yum -y groupinstall ohpc-parallel-libs-gnu-mpich
     yum -y groupinstall ohpc-parallel-libs-gnu-mvapich2
     yum -y groupinstall ohpc-parallel-libs-gnu-openmpi
elif [[ ${enable_mpi_opa} -eq 1 ]];then
     yum -y groupinstall ohpc-parallel-libs-gnu-mvapich2
     yum -y groupinstall ohpc-parallel-libs-gnu-openmpi
fi

# -----------------------------------------------------------------------------------
# Install Optional Development Tools for use with Intel Parallel Studio (Section 4.7)
# -----------------------------------------------------------------------------------
if [[ ${enable_intel_packages} -eq 1 ]];then
     yum -y install intel-compilers-devel-ohpc
     yum -y install intel-mpi-devel-ohpc
     if [[ ${enable_mpi_opa} -eq 1 ]];then
          yum -y install openmpi-psm2-intel-ohpc mvapich2-psm2-intel-ohpc
     fi
     yum -y groupinstall ohpc-serial-libs-intel
     yum -y groupinstall ohpc-io-libs-intel
     yum -y groupinstall ohpc-perf-tools-intel
     yum -y groupinstall ohpc-python-libs-intel
     yum -y groupinstall ohpc-runtimes-intel
     yum -y groupinstall ohpc-parallel-libs-intel-mpich
     yum -y groupinstall ohpc-parallel-libs-intel-mvapich2
     yum -y groupinstall ohpc-parallel-libs-intel-openmpi
     yum -y groupinstall ohpc-parallel-libs-intel-impi
fi

# -------------------------------------------------------------
# Allow for optional sleep to wait for provisioning to complete
# -------------------------------------------------------------
sleep ${provision_wait}

# ------------------------------------
# Resource Manager Startup (Section 5)
# ------------------------------------
systemctl enable munge
systemctl enable slurmctld
systemctl start munge
systemctl start slurmctld
pdsh -w c[1-4] systemctl start slurmd
useradd -m test
wwsh file resync passwd shadow group
sleep 2
pdsh -w c[1-4] /warewulf/bin/wwgetfiles </code></pre>
</section><section id="and-input.local" class="slide level2">
<h2>and input.local</h2>
<pre><code>
# ---------------------------
# SMS (master) node settings
# ---------------------------

# Hostname for master server (SMS)
sms_name=&quot;${sms_name:-67}&quot;

# Local (internal) IP address on SMS
sms_ip=&quot;${sms_ip:-12.23.45.67}&quot;

# Internal ethernet interface on SMS
sms_eth_internal=&quot;${sms_eth_internal:-nic1}&quot;

# Subnet netmask for internal cluster network
internal_netmask=&quot;${internal_netmask:-255.255.252.0}&quot;

# Local ntp server for time synchronization
ntp_server=&quot;${ntp_server:-ntp.domain.dk}&quot;

# BMC user credentials for use by IPMI
bmc_username=&quot;${bmc_username:-HackMeNow}&quot;
bmc_password=&quot;${bmc_password:-CloseButNoCigar}&quot;

# Additional time to wait for compute nodes to provision (seconds)
provision_wait=&quot;${provision_wait:-180}&quot;

# Provisioning interface used by compute hosts (Warewulf recipe only)
eth_provision=&quot;${eth_provision:-nic1}&quot;

# Flags for optional installation/configuration
enable_ib=&quot;${enable_ib:-0}&quot;
enable_opa=&quot;${enable_opa:-0}&quot;
enable_opafm=&quot;${enable_opafm:-0}&quot;
enable_mpi_defaults=&quot;${enable_mpi_defaults:-1}&quot;
enable_ipoib=&quot;${enable_ipoib:-0}&quot;

enable_clustershell=&quot;${enable_clustershell:-1}&quot;
enable_ipmisol=&quot;${enable_ipmisol:-1}&quot;
enable_opensm=&quot;${enable_opensm:-0}&quot;
enable_ganglia=&quot;${enable_ganglia:-1}&quot;
enable_genders=&quot;${enable_genders:-1}&quot;
enable_kargs=&quot;${enable_kargs:-0}&quot;
enable_beegfs_client=&quot;${enable_beegfs_client:-0}&quot;
enable_lustre_client=&quot;${enable_lustre_client:-0}&quot;
enable_mrsh=&quot;${enable_mrsh:-1}&quot;
enable_nagios=&quot;${enable_nagios:-1}&quot;
enable_powerman=&quot;${enable_powerman:-1}&quot;
enable_pmix=&quot;${enable_pmix:-0}&quot;
enable_intel_packages=&quot;${enable_intel_packages:-1}&quot;

nagios_web_password=&quot;${nagios_web_password:-CloseButNoCigar}&quot;
update_slurm_nodeconfig=&quot;${update_slurm_nodeconfig:-0}&quot;
slurm_node_config=&quot;${slurm_node_config:-c[1-2] Sockets=2 CoresPerSocket=10 ThreadsPerCore=1}&quot;

# -------------------------
# compute node settings
# -------------------------

# total number of computes
num_computes=&quot;${num_computes:-2}&quot;

# regex and starting prefix that matches defined compute hostnames
compute_regex=&quot;${compute_regex:-c*}&quot;
compute_prefix=&quot;${compute_prefix:-c}&quot;

# compute hostnames
c_name[0]=c1
c_name[1]=c2

# compute node IP addresses
c_ip[0]=12.34.56.79
c_ip[1]=12.34.56.80

# compute node MAC addreses for provisioning interface
c_mac[0]=de:ad:be:ef:ca:fe
c_mac[1]=de:ad:be:ef:ca:ff

# compute node BMC addresses
c_bmc[0]=22.34.56.79
c_bmc[1]=22.34.56.80

#-------------------
# Optional settings
#-------------------

# additional arguments to enable optional arguments for bootstrap kernel
kargs=&quot;${kargs:-acpi_pad.disable=1}&quot;

# BeeGFS repository URL
beegfs_repo=&quot;${beegfs_repo:-https://www.beegfs.io/release/beegfs_6/dists/beegfs-rhel7.repo}&quot;

# BeeGFS sysMgmtdHost
sysmgmtd_host=&quot;${sysmgmtd_host:-172.17.1.16}&quot;

# Lustre MGS mount name
mgs_fs_name=&quot;${mgs_fs_name:-192.168.100.254@o2ib:/lustre1}&quot;

# Subnet netmask for IPoIB network
ipoib_netmask=&quot;${ipoib_netmask:-255.255.0.0}&quot;

# IPoIB address for SMS server
sms_ipoib=&quot;${sms_ipoib:-192.168.0.1}&quot;

# IPoIB addresses for computes
c_ipoib[0]=192.168.1.79                     
c_ipoib[1]=192.168.1.80</code></pre>
</section><section id="roll-with-ansible-roles" class="slide level2">
<h2>Roll with Ansible roles</h2>
<figure>
<img src="./figures/playbook-ohpc.png" width="500" />
</figure>
<ul>
<li>--tags and --skip-tags</li>
</ul>
</section><section id="warewulf" class="slide level2">
<h2>Warewulf</h2>
<ul>
<li>Warewulf is a scalable systems management suite
<ul>
<li>easy</li>
<li>open</li>
<li>scalable</li>
<li>.... sold</li>
</ul></li>
</ul>
<figure>
<img src="./figures/warewulf-goals.png" width="600" />
</figure>
</section><section id="warewulf-administration" class="slide level2">
<h2>Warewulf administration</h2>
<ul>
<li>pxe/tftp</li>
<li>bootstrap</li>
<li><a href="http://warewulf.lbl.gov/subprojects_components_plugins/vnfs.html">VNFS</a> chroot image</li>
<li>stateless (in RAM + VNFS)</li>
<li>stateful (on local disk)</li>
</ul>
<p>Commands</p>
<pre class="code"><code>            wwsh bootstrap list
            wwsh provision list
            wwsh file list 
            wwsh node list 
            wwsh object list 
            wwsh vnfs list 


            wwsh provision set c23 --vnfs=centos7.5 --bootstrap=2.6.32-71.18.2.el6.x86_64

            wwsh node new c23 --netdev=em1 --ipaddr=10.40.85.23 --netmask=255.255.252.0 \
                              --gateway=10.40.84.1 --hwaddr=d4:ae:52:8a:7e:6b
</code></pre>
</section><section id="openhpc-components" class="slide level2">
<h2>OpenHPC components</h2>
<figure>
<img src="./figures/components.png" />
</figure>
<ul>
<li><a href="https://github.com/openhpc/ohpc/wiki/Component-List-v1.3.5" class="uri">https://github.com/openhpc/ohpc/wiki/Component-List-v1.3.5</a></li>
</ul>
</section></section>
<section data-background="style/background.svg"><section id="components" class="titleslide slide level1"><h1>Components</h1></section><section id="administrative-tools" class="slide level2">
<h2>Administrative Tools</h2>
<ul>
<li><a href="http://clustershell.sourceforge.net"><strong><code>clustershell</code></strong></a></li>
<li><a href="http://dun.github.io/conman"><strong><code>conman</code></strong></a> serial console management and control</li>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>docs</code></strong></a></li>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>examples</code></strong></a></li>
<li><a href="https://github.com/chaos/genders"><strong><code>genders</code></strong></a></li>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>lmod-defaults</code></strong></a></li>
<li><a href="https://github.com/TACC/Lmod"><strong><code>lmod</code></strong></a> module environment : <code>module load intel/2018b</code></li>
<li><a href="https://github.com/hpcsi/losf"><strong><code>losf</code></strong></a></li>
<li><a href="https://github.com/chaos/mrsh"><strong><code>mrsh</code></strong></a> local login via munge : <code>mrsh n001</code></li>
<li><a href="http://sourceforge.net/projects/pdsh"><strong><code>pdsh</code></strong></a> parallel commands : <code>pdsh -w n[001-100] date</code></li>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>prun</code></strong></a> parallel execution</li>
<li><a href="https://github.com/openhpc/ohpc/tests"><strong><code>test-suite</code></strong></a></li>
</ul>
</section><section id="monitoring" class="slide level2">
<h2>Monitoring</h2>
<ul>
<li><a href="http://ganglia.sourceforge.net"><strong><code>ganglia</code></strong></a> cluster metrics</li>
<li><a href="http://www.nagios.org"><strong><code>nagios</code></strong></a> server monitoring</li>
<li><a href="https://www.nagios-plugins.org"><strong><code>nagios-plugins</code></strong></a></li>
<li><a href="http://www.nagios.org/download/addons"><strong><code>ndoutils</code></strong></a> multi nagios + history</li>
<li><a href="https://github.com/mej/nhc"><strong><code>nhc</code></strong></a> node health check</li>
<li><a href="http://www.nagios.org"><strong><code>nrpe</code></strong></a> remote execution</li>
</ul>
</section><section id="compiler-families" class="slide level2">
<h2>Compiler Families</h2>
<ul>
<li><a href="http://gcc.gnu.org"><strong><code>Gnu Compiler Suite</code></strong></a></li>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>Intel Compiler Compatibility Package</code></strong></a></li>
<li><a href="http://www.llvm.org"><strong><code>LLVM Compiler Suite</code></strong></a></li>
</ul>
</section><section id="development-tools" class="slide level2">
<h2>Development Tools</h2>
<ul>
<li><a href="http://www.gnu.org/software/autoconf"><strong><code>autoconf</code></strong></a></li>
<li><a href="http://www.gnu.org/software/automake"><strong><code>automake</code></strong></a></li>
<li><a href="https://cmake.org"><strong><code>cmake</code></strong></a></li>
<li><a href="http://www.open-mpi.org/projects/hwloc"><strong><code>hwloc</code></strong></a></li>
<li><a href="http://www.gnu.org/software/libtool"><strong><code>libtool</code></strong></a></li>
<li><a href="https://bitbucket.org/mpi4py/mpi4py"><strong><code>python-mpi4py</code></strong></a></li>
<li><a href="http://sourceforge.net/projects/numpy"><strong><code>python-numpy</code></strong></a></li>
<li><a href="http://www.scipy.org"><strong><code>python-scipy</code></strong></a></li>
<li><a href="http://www.valgrind.org"><strong><code>valgrind</code></strong></a></li>
</ul>
</section><section id="hpc-package-systems" class="slide level2">
<h2>HPC package systems</h2>
<ul>
<li><a href="http://easybuilders.github.io/easybuild"><strong><code>Anaconda</code></strong></a></li>
<li><a href="http://easybuilders.github.io/easybuild"><strong><code>EasyBuild</code></strong></a></li>
<li><a href="https://github.com/LLNL/spack"><strong><code>Spack</code></strong></a></li>
</ul>
</section><section id="updates-of-distro-provided-packages" class="slide level2">
<h2>Updates of Distro-provided packages:</h2>
<ul>
<li><a href="http://bitop.luajit.org"><strong><code>lua-bit</code></strong></a></li>
<li><a href="http://keplerproject.github.com/luafilesystem"><strong><code>lua-filesystem</code></strong></a></li>
<li><a href="https://github.com/luaposix/luaposix"><strong><code>lua-posix</code></strong></a></li>
</ul>
</section><section id="io-libraries" class="slide level2">
<h2>IO Libraries:</h2>
<ul>
<li><a href="http://www.olcf.ornl.gov/center-projects/adios"><strong><code>adios</code></strong></a></li>
<li><a href="http://www.hdfgroup.org/HDF5"><strong><code>hdf5</code></strong></a></li>
<li><a href="http://www.unidata.ucar.edu/software/netcdf"><strong><code>netcdf-cxx</code></strong></a></li>
<li><a href="http://www.unidata.ucar.edu/software/netcdf"><strong><code>netcdf-fortran</code></strong></a></li>
<li><a href="http://www.unidata.ucar.edu/software/netcdf"><strong><code>netcdf</code></strong></a></li>
<li><a href="http://www.hdfgroup.org/HDF5"><strong><code>phdf5</code></strong></a></li>
<li><a href="http://cucis.ece.northwestern.edu/projects/PnetCDF"><strong><code>pnetcdf</code></strong></a></li>
<li><a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/SIONlib/_node.html"><strong><code>sionlib</code></strong></a></li>
</ul>
</section><section id="serial-threaded-libraries" class="slide level2">
<h2>Serial / Threaded Libraries</h2>
<ul>
<li><a href="http://www.r-project.org"><strong><code>R</code></strong></a> statistical computing and graphics</li>
<li><a href="http://www.gnu.org/software/gsl"><strong><code>gsl</code></strong></a> numerical library</li>
<li><a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview"><strong><code>metis</code></strong></a> partitioning graph / finite element meshes</li>
<li><a href="http://www.openblas.net"><strong><code>openblas</code></strong></a> optimized linear algebra</li>
<li><a href="https://bitbucket.org/icl/plasma"><strong><code>plasma</code></strong></a> multi core parallel linear algebra</li>
<li><a href="http://crd.lbl.gov/~xiaoye/SuperLU"><strong><code>superlu</code></strong></a> large, sparse, nonsymmetric systems</li>
</ul>
</section><section id="mpi-runtime-families" class="slide level2">
<h2>MPI Runtime Families</h2>
<ul>
<li><a href="https://github.com/openhpc/ohpc"><strong><code>Intel MPI Compatibility Package</code></strong></a></li>
<li><a href="http://www.mpich.org"><strong><code>mpich</code></strong></a></li>
<li><a href="http://mvapich.cse.ohio-state.edu/overview/mvapich2"><strong><code>mvapich2</code></strong></a></li>
<li><a href="http://www.open-mpi.org"><strong><code>openmpi</code></strong></a></li>
</ul>
</section><section id="parallel-libraries" class="slide level2">
<h2>Parallel Libraries</h2>
<ul>
<li><a href="http://www.boost.org"><strong><code>boost</code></strong></a></li>
<li><a href="http://www.fftw.org"><strong><code>fftw</code></strong></a></li>
<li><a href="http://www.llnl.gov/casc/hypre"><strong><code>hypre</code></strong></a></li>
<li><a href="http://mfem.org"><strong><code>mfem</code></strong></a></li>
<li><a href="http://mumps.enseeiht.fr"><strong><code>mumps</code></strong></a></li>
<li><a href="http://www.mcs.anl.gov/petsc"><strong><code>petsc</code></strong></a></li>
<li><a href="http://www.labri.fr/perso/pelegrin/scotch"><strong><code>scotch</code></strong></a></li>
<li><a href="http://slepc.upv.es"><strong><code>slepc</code></strong></a></li>
<li><a href="http://crd-legacy.lbl.gov/~xiaoye/SuperLU"><strong><code>superlu_dist</code></strong></a></li>
<li><a href="http://trilinos.sandia.gov/index.html"><strong><code>trilinos</code></strong></a></li>
</ul>
</section><section id="performance-tools" class="slide level2">
<h2>Performance Tools</h2>
<ul>
<li><a href="https://software.intel.com/en-us/articles/intel-mpi-benchmarks"><strong><code>imb</code></strong></a></li>
<li><a href="https://github.com/RRZE-HPC/likwid"><strong><code>likwid</code></strong></a></li>
<li><a href="http://mpip.sourceforge.net"><strong><code>mpiP</code></strong></a></li>
<li><a href="http://icl.cs.utk.edu/papi"><strong><code>papi</code></strong></a></li>
<li><a href="http://www.cs.uoregon.edu/Research/pdt"><strong><code>pdtoolkit</code></strong></a></li>
<li><a href="http://www.scalasca.org"><strong><code>scalasca</code></strong></a></li>
<li><a href="http://www.vi-hps.org/projects/score-p"><strong><code>scorep</code></strong></a></li>
<li><a href="http://www.cs.uoregon.edu/research/tau/home.php"><strong><code>tau</code></strong></a></li>
</ul>
</section><section id="resource-management" class="slide level2">
<h2>Resource Management</h2>
<ul>
<li><a href="https://slurm.schedmd.com"><strong><code>slurm</code></strong></a></li>
<li><a href="https://github.com/PBSPro/pbspro"><strong><code>PBSPro</code></strong></a></li>
<li><a href="http://dun.github.io/munge"><strong><code>munge</code></strong></a></li>
<li><a href="https://pmix.github.io/pmix"><strong><code>pmix</code></strong></a></li>
</ul>
</section><section id="high-performance-file-systems" class="slide level2">
<h2>High performance file systems</h2>
<ul>
<li><a href="https://wiki.hpdd.intel.com"><strong><code>lustre-client</code></strong></a></li>
<li><a href="http://lustre-shine.sourceforge.net"><strong><code>shine</code></strong></a></li>
</ul>
</section><section id="containers-in-userland" class="slide level2">
<h2>Containers in userland</h2>
<ul>
<li><a href="https://hpc.github.io/charliecloud"><strong><code>charliecloud</code></strong></a></li>
<li><a href="https://xstack.exascale-tech.com/wiki"><strong><code>ocr</code></strong></a></li>
<li><a href="http://singularity.lbl.gov"><strong><code>singularity</code></strong></a></li>
</ul>
</section></section>
    </div>
  </div>

  <script src="revealjs/lib/js/head.min.js"></script>
  <script src="revealjs/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: false,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Flags if speaker notes should be visible to all viewers
        showNotes: false,
        // Opens links in an iframe preview overlay
        previewLinks: false,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow

        // Optional reveal.js plugins
        dependencies: [
          { src: 'revealjs/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'revealjs/plugin/zoom-js/zoom.js', async: true },
              { src: 'revealjs/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
